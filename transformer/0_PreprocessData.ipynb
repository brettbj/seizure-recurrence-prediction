{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('../creds.txt', 'r') as file:\n",
    "    creds = file.read()\n",
    "\n",
    "cn = create_engine(f\"postgresql://bch:{creds}@compute-e-16-229:54320/eps\", \n",
    "                     connect_args={'options': '-csearch_path={}'.format('bch')}).execution_options(autocommit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get patients to exclude - which don't match inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "days_before = 365\n",
    "days_after = 365*2\n",
    "min_age = 2\n",
    "min_year = 2000\n",
    "req_freq = 0.0001\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pat_query = f\"\"\"\n",
    "SELECT patient_num FROM coverage_time \n",
    "WHERE (days_before_eps > {days_before} or age_onset < {min_age}) AND (days_after_eps > {days_after})\n",
    "AND eps_onset_date BETWEEN TO_DATE('01-01-{min_year}','DD-MM-YY') AND TO_DATE('01-01-2020','DD-MM-YY')\n",
    "AND age_onset BETWEEN 0 AND 21\n",
    "\"\"\"\n",
    "\n",
    "exclude_pat_df = pd.read_sql(pat_query, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = f\"\"\"SELECT * FROM information_schema.tables WHERE table_schema = 'bch';\"\"\"\n",
    "# pd.read_sql(query, cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pat_query = f\"\"\"\n",
    "SELECT patient_num \n",
    "FROM patient_dimension where patient_num NOT IN ({pat_query})\"\"\"\n",
    "include_pat_df = pd.read_sql(train_pat_query, cn)\n",
    "include_pat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# then select notes from other patients\n",
    "query = f\"\"\"SELECT * FROM CONCEPT_DIMENSION WHERE concept_cd = 'NOTE:3268562'\"\"\"\n",
    "pd.read_sql(query, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popular_note_query = f\"\"\"\n",
    "SELECT n.concept_cd, count(1) \n",
    "FROM notes n \n",
    "JOIN ({pat_query}) p\n",
    "ON n.patient_num = p.patient_num\n",
    "GROUP BY n.concept_cd\n",
    "\"\"\"\n",
    "\n",
    "# popular_notes = pd.read_sql(popular_note_query, cn)\n",
    "# popular_notes.to_csv('./resources/popular_notes.csv', index=False)\n",
    "popular_notes = pd.read_csv('../resources/popular_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popular_notes.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "top_rows = popular_notes.sort_values(by='count', ascending=False)\n",
    "\n",
    "j = 0\n",
    "for i, row in top_rows.iterrows():\n",
    "    j+=1\n",
    "    path_to_file = f\"/n/data1/hms/dbmi/beaulieu-jones/lab/epilepsy-transformer/raw/{row['concept_cd']}.txt\"\n",
    "    \n",
    "    file_exists = exists(path_to_file)\n",
    "    # print(file_exists)\n",
    "    if not file_exists:\n",
    "        print(f\"{path_to_file} doesn't exist\")\n",
    "    if j > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pop_note_list = popular_notes[popular_notes['count']>10].sort_values(by='count', ascending=False)['concept_cd'].tolist()\n",
    "pop_note_list = popular_notes[popular_notes['count']>10]['concept_cd'].tolist()\n",
    "pop_note_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = f\"\"\"SELECT * FROM CONCEPT_DIMENSION WHERE CONCEPT_CD ='NOTE:15611138'\"\"\"\n",
    "# pd.read_sql(query, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import swifter\n",
    "tqdm.pandas()\n",
    "import time\n",
    "\n",
    "note_limit = 1000000\n",
    "OUTPUT_DIR = '/n/data1/hms/dbmi/beaulieu-jones/lab/epilepsy-transformer/' \n",
    "\n",
    "i=0\n",
    "for concept_cd in pop_note_list:\n",
    "    output_file_name = f\"{OUTPUT_DIR}/raw/{concept_cd}.txt\"\n",
    "    i+=1\n",
    "    \n",
    "    file_exists = exists(output_file_name)\n",
    "    if file_exists:\n",
    "        print('file exists')\n",
    "    else:\n",
    "        print(i, concept_cd)\n",
    "        \n",
    "        note_query = f\"\"\"\n",
    "        SELECT encounter_num, n.patient_num, n.concept_cd, observation_blob, note_id\n",
    "        FROM notes n \n",
    "        JOIN ({train_pat_query}) p\n",
    "        ON n.patient_num = p.patient_num\n",
    "        WHERE n.concept_cd = '{concept_cd}' AND length(n.observation_blob) > 50\n",
    "        LIMIT {note_limit}\n",
    "        \"\"\"\n",
    "        # print(note_query)\n",
    "\n",
    "        before = time.time()\n",
    "        notes = pd.read_sql(note_query, cn)\n",
    "        notes['text'] = notes['observation_blob']\n",
    "        print(concept_cd, notes.shape)\n",
    "        if notes.shape[0] > 10:\n",
    "            notes.to_csv(output_file_name, index=False)\n",
    "        after = time.time()\n",
    "        print(f\"{i} / {len(pop_note_list)} - {after-before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import swifter\n",
    "tqdm.pandas()\n",
    "import time\n",
    "note_limit = 1000\n",
    "\n",
    "import os\n",
    "\n",
    "i = 0\n",
    "for concept_cd in pop_note_list:\n",
    "    category = f'{concept_cd}'\n",
    "    output_file_name = f\"{OUTPUT_DIR}/raw/{category}.txt\"\n",
    "    i+=1\n",
    "    \n",
    "    file_exists = os.path.exists(output_file_name)\n",
    "    if file_exists:\n",
    "        print('file exists')\n",
    "        continue\n",
    "    \n",
    "    note_query = f\"\"\"\n",
    "SELECT encounter_num, n.patient_num, n.concept_cd, observation_blob, note_id, \n",
    "cd.name_char, cd.concept_path \n",
    "FROM notes n \n",
    "JOIN ({train_pat_query}) p\n",
    "ON n.patient_num = p.patient_num\n",
    "JOIN CONCEPT_DIMENSION cd ON \n",
    "cd.concept_cd = n.concept_cd\n",
    "WHERE n.concept_cd = '{concept_cd}' AND length(n.observation_blob) > 50\n",
    "LIMIT {note_limit}\n",
    "\"\"\"\n",
    "    \n",
    "    before = time.time()\n",
    "    notes = pd.read_sql(note_query, cn)\n",
    "    notes['text'] = notes['observation_blob']\n",
    "    notes.to_csv(output_file_name, index=False)\n",
    "    after = time.time()\n",
    "    print(f\"{i} / {len(pop_note_list)} - {after-before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/jobs/*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "j=0\n",
    "for concept in pop_note_list:\n",
    "    job_string = f\"\"\"#!/bin/bash\n",
    "#SBATCH -t 0-12:00\n",
    "#SBATCH -n 1\n",
    "#SBATCH -p gpu_zak\n",
    "#SBATCH\t--account=zak_contrib_isk1\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -o /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/jobs/output/{concept}_%j.out\n",
    "#SBATCH -e /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/jobs/err/{concept}_%j.err\n",
    "module load conda2\n",
    "source activate transformer\n",
    "python3 -u /home/bkb12/notebooks/transformer/process_concept.py --concept_cd '{concept}' \n",
    "    \"\"\"\n",
    "    \n",
    "    if not exists(f\"/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/preproc_sent/{concept}.txt\"):\n",
    "        j+=1\n",
    "    #     with open(f\"/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/jobs/{concept}.sh\", 'w') as f:\n",
    "    #         f.write(job_string)\n",
    "        \n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input file format:\n",
    "# (1) One sentence per line. These should ideally be actual sentences, not\n",
    "# entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "# sentence boundaries for the \"next sentence prediction\" task).\n",
    "# (2) Blank lines between documents. Document boundaries are needed so\n",
    "# that the \"next sentence prediction\" task doesn't span between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import stanfordnlp\n",
    "import time\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "from heuristic_tokenize import sent_tokenize_rules \n",
    "from spacy.language import Language\n",
    "\n",
    "OUTPUT_DIR = '/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/' #this path will contain tokenized notes. This dir will be the input dir for create_pretrain_data.sh\n",
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner'])\n",
    "nlp.add_pipe('sbd_component', first=True) \n",
    "\n",
    "#setting sentence boundaries\n",
    "@Language.component('sbd_component')\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc\n",
    "\n",
    "#convert de-identification text into one token\n",
    "def fix_deid_tokens(text, processed_text):\n",
    "    deid_regex  = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\" \n",
    "    if text:\n",
    "        indexes = [m.span() for m in re.finditer(deid_regex,text,flags=re.IGNORECASE)]\n",
    "    else:\n",
    "        indexes = []\n",
    "    for start,end in indexes:\n",
    "        processed_text.merge(start_idx=start,end_idx=end)\n",
    "    return processed_text\n",
    "    \n",
    "\n",
    "def process_section(section, note, processed_sections):\n",
    "    # perform spacy processing on section\n",
    "    processed_section = nlp(section['sections'])\n",
    "    processed_section = fix_deid_tokens(section['sections'], processed_section)\n",
    "    processed_sections.append(processed_section)\n",
    "\n",
    "def process_note_helper(note):\n",
    "    # split note into sections\n",
    "    note_sections = sent_tokenize_rules(note)\n",
    "    processed_sections = []\n",
    "    section_frame = pd.DataFrame({'sections':note_sections})\n",
    "    section_frame.apply(process_section, args=(note,processed_sections,), axis=1)\n",
    "    return(processed_sections)\n",
    "\n",
    "def process_text(sent, note):\n",
    "    sent_text = sent['sents'].text\n",
    "    if len(sent_text) > 0 and sent_text.strip() != '\\n':\n",
    "        if '\\n' in sent_text:\n",
    "            sent_text = sent_text.replace('\\n', ' ')\n",
    "        note['text'] += sent_text + '\\n'  \n",
    "\n",
    "def get_sentences(processed_section, note):\n",
    "    # get sentences from spacy processing\n",
    "    sent_frame = pd.DataFrame({'sents': list(processed_section['sections'].sents)})\n",
    "    sent_frame.apply(process_text, args=(note,), axis=1)\n",
    "\n",
    "def process_note(note):\n",
    "    try:\n",
    "        note_text = note['text'] \n",
    "        note['text'] = ''\n",
    "        processed_sections = process_note_helper(note_text)\n",
    "        ps = {'sections': processed_sections}\n",
    "        ps = pd.DataFrame(ps)\n",
    "        ps.apply(get_sentences, args=(note,), axis=1)\n",
    "        return note \n",
    "    except Exception as e:\n",
    "        # pass\n",
    "        print ('error', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import swifter\n",
    "tqdm.pandas()\n",
    "import time\n",
    "note_limit = 100000000\n",
    "\n",
    "import os\n",
    "\n",
    "i = 0\n",
    "for concept_cd in pop_note_list:\n",
    "    category = f'{concept_cd}'\n",
    "    output_file_name = f\"{OUTPUT_DIR}{category}.txt\"\n",
    "    i+=1\n",
    "    \n",
    "    file_exists = os.path.exists(output_file_name)\n",
    "    if file_exists:\n",
    "        continue\n",
    "    \n",
    "    note_query = f\"\"\"\n",
    "SELECT encounter_num, n.patient_num, n.concept_cd, observation_blob, note_id, \n",
    "cd.name_char, cd.concept_path \n",
    "FROM notes n \n",
    "JOIN ({train_pat_query}) p\n",
    "ON n.patient_num = p.patient_num\n",
    "JOIN CONCEPT_DIMENSION cd ON \n",
    "cd.concept_cd = n.concept_cd\n",
    "WHERE n.concept_cd = '{concept_cd}' AND length(n.observation_blob) > 50\n",
    "LIMIT {note_limit}\n",
    "\"\"\"\n",
    "    \n",
    "    before = time.time()\n",
    "    notes = pd.read_sql(note_query, cn)\n",
    "    notes['text'] = notes['observation_blob']\n",
    "    # display(notes)\n",
    "    after = time.time()\n",
    "    print(f\"{concept_cd} ({i}/{len(pop_note_list)}): {after-before}\")\n",
    "    print('Number of notes: %d' %len(notes.index))\n",
    "    notes['ind'] = list(range(len(notes.index)))\n",
    "    formatted_notes = notes.progress_apply(process_note, axis=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    if formatted_notes.shape[0] > 0:\n",
    "        \n",
    "        print(output_file_name)\n",
    "        with open(output_file_name,'w') as f:\n",
    "            for text in formatted_notes['text']:\n",
    "                if text != None and len(text) != 0 :\n",
    "                    f.write(text)\n",
    "                    f.write('\\n')\n",
    "\n",
    "        end = time.time()\n",
    "        print (end-start)\n",
    "        print (\"Done formatting notes\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "note_query = \"SELECT * FROM notes n WHERE n.concept_cd IN ('NOTE:15611138') LIMIT 10\"\n",
    "notes = pd.read_sql(note_query, cn)\n",
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total count - 4,711,809\n",
    "\n",
    "\n",
    "# note_query = f\"\"\"\n",
    "# SELECT encounter_num, n.patient_num, n.concept_cd, observation_blob, note_id, \n",
    "# cd.name_char, cd.concept_path FROM notes n \n",
    "# JOIN ({train_pat_query}) p\n",
    "# ON n.patient_num = p.patient_num\n",
    "# JOIN CONCEPT_DIMENSION cd ON \n",
    "# cd.concept_cd = n.concept_cd\n",
    "# WHERE n.concept_cd NOT IN ('NOTE:3691317', 'NOTE:3268562')\n",
    "# LIMIT {note_limit}\n",
    "# \"\"\"\n",
    "\n",
    "note_limit = 20\n",
    "\n",
    "note_query = f\"\"\"\n",
    "SELECT encounter_num, n.patient_num, n.concept_cd, observation_blob, note_id, \n",
    "cd.name_char, cd.concept_path \n",
    "FROM notes n \n",
    "JOIN ({train_pat_query}) p\n",
    "ON n.patient_num = p.patient_num\n",
    "JOIN CONCEPT_DIMENSION cd ON \n",
    "cd.concept_cd = n.concept_cd\n",
    "WHERE n.concept_cd IN ('NOTE:3268565')\n",
    "LIMIT {note_limit}\n",
    "\"\"\"\n",
    "\n",
    "notes = pd.read_sql(note_query, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# notes = notes[notes['category'] == category]\n",
    "category = f'notelim_{note_limit}'\n",
    "print('Number of notes: %d' %len(notes.index))\n",
    "notes['ind'] = list(range(len(notes.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"spacy\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.autonotebook import tqdm\n",
    "# import swifter\n",
    "# formatted_notes = notes.swifter.apply(process_note, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "formatted_notes = notes.progress_apply(process_note, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR  + category + '.txt','w') as f:\n",
    "    for text in formatted_notes['text']:\n",
    "        if text != None and len(text) != 0 :\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "end = time.time()\n",
    "print (end-start)\n",
    "print (\"Done formatting notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/preproc_sent\"\n",
    "\n",
    "filename_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    # print(filename[:-4])\n",
    "    filename_list.append(filename[:-4])\n",
    "    \n",
    "# joined_string = \", \".join(filename_list)\n",
    "# print(joined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/bert_preprocess_jobs/*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "j=0\n",
    "\n",
    "for concept in filename_list:\n",
    "    job_string = f\"\"\"#!/bin/bash\n",
    "#SBATCH -t 0-12:00\n",
    "#SBATCH -n 1\n",
    "#SBATCH -p gpu_zak\n",
    "#SBATCH\t--account=zak_contrib_isk1\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH -o /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/bert_preprocess_jobs/output/{concept}_%j.out\n",
    "#SBATCH -e /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/bert_preprocess_jobs/err/{concept}_%j.err\n",
    "module load conda2 gcc/9.2.0 cuda/11.2\n",
    "source activate tf\n",
    "python3 /home/bkb12/notebooks/transformer/create_pretraining_data.py --input_file /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/preproc_sent/{concept}.txt --output_file /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/pretraining_data/{concept}.txt  --vocab_file /home/bkb12/notebooks/transformer/resources/vocab.txt \n",
    "    \"\"\"\n",
    "    \n",
    "    if not exists(f'/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/pretraining_data/{concept}.txt'):\n",
    "        j+=1\n",
    "        with open(f'/n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/bert_preprocess_jobs/{concept}.sh', 'w') as f:\n",
    "            f.write(job_string)\n",
    "        \n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /n/data1/hms/dbmi/beaulieu-jones/lab/transformer_training_data/pretraining_data/ | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
