{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ec8bd-8633-412a-8d0d-55386cd8ac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "neurology_notes = ['NOTE:125942859', 'NOTE:2820514', 'NOTE:4127998', 'NOTE:4127189', 'NOTE:3817544', 'NOTE:4776343', 'NOTE:67621192', 'NOTE:4127201', 'NOTE:4128000', 'NOTE:4127729', 'NOTE:4127835', 'NOTE:4127923', 'NOTE:4293224', 'NOTE:4127989', 'NOTE:4127595', 'NOTE:4776342', 'NOTE:4293221', 'NOTE:4633365', 'NOTE:4127190', 'NOTE:4127226', 'NOTE:4127203', 'NOTE:4127515', 'NOTE:4127192', 'NOTE:4127524', 'NOTE:4127586', 'NOTE:4293218', 'NOTE:3827999', 'NOTE:4127522', 'NOTE:4127837', 'NOTE:4127194', 'NOTE:599038309', 'NOTE:4127597', 'NOTE:4127999', 'NOTE:4293225', 'NOTE:4127992', 'NOTE:3828001', 'NOTE:4293226', 'NOTE:3755582', 'NOTE:164478027', 'NOTE:125943072', 'NOTE:4776258', 'NOTE:4127227', 'NOTE:599038319', 'NOTE:4127202', 'NOTE:490360275', 'NOTE:4127457', 'NOTE:22652867', 'NOTE:16187985', 'NOTE:400739364', 'NOTE:4776344', 'NOTE:4776072', 'NOTE:4127995', 'NOTE:4128007', 'NOTE:4127204', 'NOTE:4127191', 'NOTE:4127195']\n",
    "discharge_notes = ['NOTE:3710478', 'NOTE:67621090', 'NOTE:15612283', 'NOTE:15585515', 'NOTE:3710477', 'NOTE:15587049', 'NOTE:15587406', 'NOTE:15612287', 'NOTE:189094536', 'NOTE:15588370', 'NOTE:15588192', 'NOTE:67621087', 'NOTE:15586337', 'NOTE:153048767', 'NOTE:15587712', 'NOTE:189094539', 'NOTE:97195725', 'NOTE:618403281', 'NOTE:122778232']\n",
    "eeg_notes = ['NOTE:3691367', 'NOTE:3691278', 'NOTE:4120166', 'NOTE:4120168', 'NOTE:3691374', 'NOTE:4120169']\n",
    "mri_notes = ['NOTE:83328334', 'NOTE:83328424', 'NOTE:3447280', 'NOTE:83328328', 'NOTE:83328617', 'NOTE:83328304', 'NOTE:64217783', 'NOTE:251612637', 'NOTE:251612643', 'NOTE:83328337', 'NOTE:3447282', 'NOTE:83328313', 'NOTE:64217795', 'NOTE:83328427', 'NOTE:83328433', 'NOTE:3447264', 'NOTE:3447296', 'NOTE:64217779', 'NOTE:64217573', 'NOTE:8007688', 'NOTE:3447291', 'NOTE:202592750', 'NOTE:83328370', 'NOTE:83328406', 'NOTE:3447318', 'NOTE:83328325', 'NOTE:64217811', 'NOTE:83328403', 'NOTE:64217538', 'NOTE:83328436', 'NOTE:83328322', 'NOTE:4106734', 'NOTE:83328385', 'NOTE:4106510', 'NOTE:64217571', 'NOTE:64217568', 'NOTE:3447321', 'NOTE:3447275', 'NOTE:3447320', 'NOTE:8007690', 'NOTE:64217799', 'NOTE:83328448', 'NOTE:64217642', 'NOTE:83328292', 'NOTE:251612640', 'NOTE:3447290', 'NOTE:3447301', 'NOTE:3447317', 'NOTE:83328621', 'NOTE:83328412', 'NOTE:83328367', 'NOTE:3447300', 'NOTE:333730849', 'NOTE:3447289', 'NOTE:11921317', 'NOTE:3447293', 'NOTE:83328382', 'NOTE:100343890', 'NOTE:4106495', 'NOTE:83328280', 'NOTE:83328286', 'NOTE:8007698', 'NOTE:3447292', 'NOTE:83328400', 'NOTE:3447306', 'NOTE:8007683', 'NOTE:64217638', 'NOTE:3447266', 'NOTE:3447283', 'NOTE:345927837', 'NOTE:83328379', 'NOTE:83328331', 'NOTE:3447322', 'NOTE:83328373', 'NOTE:3447323', 'NOTE:64217409', 'NOTE:83328363', 'NOTE:64217323', 'NOTE:3447260', 'NOTE:3447313', 'NOTE:64217561', 'NOTE:8007708', 'NOTE:83328298', 'NOTE:8007672', 'NOTE:83328430', 'NOTE:3447308', 'NOTE:3447285', 'NOTE:345927816', 'NOTE:3447314', 'NOTE:4106731', 'NOTE:333730843', 'NOTE:83328301', 'NOTE:64217416', 'NOTE:64217539', 'NOTE:64217666', 'NOTE:64217530', 'NOTE:64217393', 'NOTE:64217384', 'NOTE:8007725', 'NOTE:3447302', 'NOTE:11921316', 'NOTE:3447258']\n",
    "ct_notes = ['NOTE:3446996', 'NOTE:3447017', 'NOTE:202592738', 'NOTE:3446983', 'NOTE:3446993', 'NOTE:3447020', 'NOTE:3446980', 'NOTE:3447006', 'NOTE:3446997', 'NOTE:3446998', 'NOTE:4873824', 'NOTE:3446982', 'NOTE:230601754', 'NOTE:3446974', 'NOTE:202592729', 'NOTE:3447012', 'NOTE:3446985', 'NOTE:3446999', 'NOTE:64217632', 'NOTE:4873823', 'NOTE:3447011', 'NOTE:3447004', 'NOTE:230601760', 'NOTE:428074121', 'NOTE:244897858', 'NOTE:64217494', 'NOTE:3446990', 'NOTE:463436530', 'NOTE:64217627', 'NOTE:3446991', 'NOTE:64217513', 'NOTE:64217581', 'NOTE:3447015', 'NOTE:8007643', 'NOTE:3447008', 'NOTE:428074112', 'NOTE:452620691', 'NOTE:64217599', 'NOTE:83328241', 'NOTE:3446979', 'NOTE:3447018', 'NOTE:64217357', 'NOTE:3446995']\n",
    "all_relevant = neurology_notes + discharge_notes + eeg_notes + mri_notes + ct_notes\n",
    "\n",
    "note_cat_dict = {\n",
    "    'neurology_notes': neurology_notes,\n",
    "    'discharge_notes': discharge_notes,\n",
    "    'eeg_notes': eeg_notes,\n",
    "    'mri_notes': mri_notes,\n",
    "    'ct_notes': ct_notes,\n",
    "    'all_relevant': all_relevant\n",
    "}\n",
    "\n",
    "frozen=0\n",
    "split_num=2\n",
    "key = 'all_relevant'\n",
    "base_path = f\"\"\"/n/data1/hms/dbmi/beaulieu-jones/lab/epilepsy-transformer/classifier-input\"\"\"\n",
    "fname = f\"{base_path}/{key}_classifier_no_filter_input.csv\"\n",
    "\n",
    "df = pd.read_csv(fname)\n",
    "input_df = df[~df['text'].isna()]\n",
    "\n",
    "label_df = pd.read_csv('./data/labels.csv')\n",
    "label_df.fillna(0, inplace=True)\n",
    "label_df.describe()\n",
    "label_cols = label_df.columns\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3cfb4-ae75-461e-b3c7-e595372efb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_sizek = 52\n",
    "max_token_length=4096\n",
    "\n",
    "training_round=2\n",
    "# checkpoint = \"checkpoint-81000\"\n",
    "max_token_length=4096\n",
    "\n",
    "base_path = f\"\"\"/n/data1/hms/dbmi/beaulieu-jones/lab/epilepsy-transformer/models/cat-scratch-longformer\"\"\"\n",
    "output_comb_dir = f\"{base_path}/clean-model\"\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "trained_tok_file = f\"{base_path}/tokenizers/bpe_no_filter_{vocab_sizek}_all_relevant.json\"\n",
    "tokenizer = Tokenizer.from_file(trained_tok_file)\n",
    "tokenizer.enable_truncation(max_length=max_token_length)\n",
    "print(tokenizer)\n",
    "\n",
    "from tokenizers.implementations import BaseTokenizer\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, model_max_length=max_token_length)\n",
    "fast_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n",
    "print(fast_tokenizer)\n",
    "\n",
    "# model_wcheckpoint = f\"{output_comb_dir}/{training_round}/{checkpoint}\"\n",
    "model_wcheckpoint = f\"{output_comb_dir}/{training_round}/\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_wcheckpoint, \n",
    "                                                           num_labels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b05383-2b76-4b91-ad22-3e1f38764e7e",
   "metadata": {},
   "source": [
    "- 3 or more ASMs classes (outside of composite score, additional panel 2 or more ASMs)\n",
    "- 2 or more ASMs simultaneously \n",
    "- Status epilepticus / Inpatient admissions for Seizures\n",
    "- Referrals to Neurology with Seizure ICD code\n",
    "- Procedure orders (EEG, MRI) after some threshold of original diagnosis (1 month-3 month)\n",
    "- Seizure-related Utilization above a threshold\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5681a3-995b-42fe-bf91-0756309c418b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric, Dataset\n",
    "\n",
    "input_df = input_df.merge(label_df, on='patient_num', how='left')\n",
    "display(input_df.columns)\n",
    "display(input_df['label'].value_counts())\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "split_series = input_df.groupby(['patient_num'])['label'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c0045-bb8e-4642-8f2f-201147be9a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_series = input_df.groupby(['patient_num'])['label'].max()\n",
    "print(type(split_series))\n",
    "pat_level_df = pd.DataFrame(split_series)\n",
    "pat_level_df.reset_index(inplace=True)\n",
    "pat_level_df.columns = ['patient_num', 'label']\n",
    "display(pat_level_df)\n",
    "\n",
    "\n",
    "splits = folds.split(np.zeros(pat_level_df['patient_num'].nunique()), pat_level_df['label'])\n",
    "\n",
    "for i in range(split_num+1):\n",
    "    train_idx, val_idx = next(splits)\n",
    "print(len(train_idx), len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ec324-9127-4528-970a-4477d34c4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sub_val_idx = val_idx[:500]\n",
    "\n",
    "dataset = Dataset.from_pandas(input_df[['text', 'patient_num', 'label']], preserve_index=False)\n",
    "\n",
    "print(len(dataset))\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) > 20)\n",
    "print(len(dataset))\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# train_idxs, val_idxs = enumerate(splits, 0)\n",
    "fold_dataset = DatasetDict({\n",
    "    \"train\":dataset.filter(lambda example: example['patient_num'] in pat_level_df.iloc[train_idx]['patient_num'].tolist(),\n",
    "                           num_proc=5),\n",
    "    \"sub_val\":dataset.filter(lambda example: example['patient_num'] in pat_level_df.iloc[sub_val_idx]['patient_num'].tolist(),\n",
    "                             num_proc=5),\n",
    "    \"val\": dataset.filter(lambda example: example['patient_num'] in pat_level_df.iloc[val_idx]['patient_num'].tolist(),\n",
    "                          num_proc=5),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c206e37-ccbb-4fb4-b7a0-1cd2441bea09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(fold_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8309f27-f4af-40b5-b799-e731ca3b0cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    examples[\"text\"] = [\n",
    "        line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    # return tokenizer.encode(''.join(examples[\"text\"]))\n",
    "    return fast_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_token_length-2,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9871f0-2e22-4d1d-a20e-7db518eb1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('train', len(fold_dataset['train']['label']), sum(fold_dataset['train']['label']))\n",
    "print('sub_val', len(fold_dataset['sub_val']['label']), sum(fold_dataset['sub_val']['label']))\n",
    "print('val', len(fold_dataset['val']['label']), sum(fold_dataset['val']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90e1d6-4bc4-4377-98c4-e67309c28f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_fold_dataset = fold_dataset.map(tokenize_function, num_proc=8, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e8197-3342-4f7c-bf43-494a3d0577cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "positive_label = tokenized_fold_dataset['train'].filter(lambda example: example['label']==1, num_proc=8) \n",
    "negative_label = tokenized_fold_dataset['train'].filter(lambda example: example['label']==0, num_proc=8)\n",
    "print(len(positive_label), len(negative_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c68e1e-edf2-47f4-a408-46f24051ce9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [42, 1234, 4567]\n",
    "\n",
    "balanced_data = None\n",
    "for s in seeds:\n",
    "        if balanced_data:\n",
    "            balanced_data = datasets.concatenate_datasets([balanced_data, \n",
    "                                                           datasets.interleave_datasets([\n",
    "                                                               positive_label.shuffle(seed=s), \n",
    "                                                               negative_label.shuffle(seed=s)]\n",
    "                                                           )]\n",
    "                                                         )\n",
    "        else:\n",
    "            balanced_data = datasets.interleave_datasets([positive_label, negative_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344ebc8-db77-4dcb-b543-f4e828f5fa99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(balanced_data), sum(balanced_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1b70e-fb7f-4edd-9356-e65b2853cd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(f\"{output_comb_dir}/{training_round}/\", num_labels=1)\n",
    "\n",
    "# metric_name = \"accuracy\"\n",
    "# model_name = f\"finetune-{label}\"\n",
    "\n",
    "model_name = f\"finetune-{label}-{key}-{frozen}-{split_num}\"\n",
    "# batch_size = 4\n",
    "base_path = f\"\"\"/n/data1/hms/dbmi/beaulieu-jones/lab/epilepsy-transformer/models/cat-scratch-longformer\"\"\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{base_path}/finetuned/{model_name}\",\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    eval_steps = 100-(50*frozen),\n",
    "    # max_eval_samples=100,\n",
    "    # save_strategy = \"epoch\",\n",
    "    # auto_find\n",
    "    per_device_train_batch_size=8+(24*frozen),\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=12,\n",
    "    # per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8+(24*frozen),\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    log_level='warning',\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # predictions = eval_pred.predictions\n",
    "    # labels = eval_pred.labels\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    roc_auc_metric = load_metric(\"roc_auc\") \n",
    "    auc = roc_auc_metric.compute(references=labels, prediction_scores=predictions)['roc_auc']\n",
    "    \n",
    "    metric1 = load_metric(\"precision\")\n",
    "    metric2 = load_metric(\"recall\")\n",
    "    \n",
    "    \n",
    "    # logits, labels = eval_pred\n",
    "    # predictions = np.argmax(logits, axis=-1)\n",
    "    binary_pred = np.where(predictions > 0.5, 1, 0)\n",
    "    \n",
    "    acc_metric = evaluate.load('accuracy') \n",
    "    acc = acc_metric.compute(predictions=binary_pred, references=labels)['accuracy']\n",
    "    \n",
    "    \n",
    "    f1_metric = load_metric(\"f1\") \n",
    "    f1 = f1_metric.compute(predictions=binary_pred, references=labels)['f1']\n",
    "    \n",
    "    precision = metric1.compute(predictions=binary_pred, references=labels)[\"precision\"]\n",
    "    recall = metric2.compute(predictions=binary_pred, references=labels)[\"recall\"]\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\":f1, \"auc\":auc, \"precision\": precision, \"recall\": recall, \n",
    "            \"pred_positive\": sum(binary_pred)[0], \"sum_positive\": sum(predictions)[0], \n",
    "            \"actual_positive\": sum(labels)}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d76d3-8975-4b66-b1c3-4b3ec6179cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # longformer.encoder.layer.11.attention.self.query.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.query.bias True\n",
    "    # longformer.encoder.layer.11.attention.self.key.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.key.bias True\n",
    "    # longformer.encoder.layer.11.attention.self.value.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.value.bias True\n",
    "    # longformer.encoder.layer.11.attention.self.query_global.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.query_global.bias True\n",
    "    # longformer.encoder.layer.11.attention.self.key_global.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.key_global.bias True\n",
    "    # longformer.encoder.layer.11.attention.self.value_global.weight True\n",
    "    # longformer.encoder.layer.11.attention.self.value_global.bias True\n",
    "    # longformer.encoder.layer.11.attention.output.dense.weight True\n",
    "    # longformer.encoder.layer.11.attention.output.dense.bias True\n",
    "    # longformer.encoder.layer.11.attention.output.LayerNorm.weight True\n",
    "    # longformer.encoder.layer.11.attention.output.LayerNorm.bias True\n",
    "    # longformer.encoder.layer.11.intermediate.dense.weight True\n",
    "    # longformer.encoder.layer.11.intermediate.dense.bias True\n",
    "    # longformer.encoder.layer.11.output.dense.weight True\n",
    "    # longformer.encoder.layer.11.output.dense.bias True\n",
    "    # longformer.encoder.layer.11.output.LayerNorm.weight True\n",
    "    # longformer.encoder.layer.11.output.LayerNorm.bias True\n",
    "    # classifier.dense.weight True\n",
    "    # classifier.dense.bias True\n",
    "    # classifier.out_proj.weight True\n",
    "    # classifier.out_proj.bias True\n",
    "    if frozen == 1:\n",
    "        if name.startswith(\"classifier\"):\n",
    "            param.requires_grad = True\n",
    "        elif name.startswith(\"longformer.encoder.layer.11\"):\n",
    "            param.requires_grad = False\n",
    "        else:    \n",
    "            param.requires_grad = False\n",
    "    else: \n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.endswith(\"dense.bias\"):\n",
    "        print(name, param.requires_grad)\n",
    "    # if name.startswith(\"classifier\"):\n",
    "    #     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431040d3-f3de-419b-9552-14704887f6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sum(fold_dataset['train']['label']), len(fold_dataset['train']['label']))\n",
    "simple_class_balance = len(fold_dataset['train']['label'])/sum(fold_dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252749f-ff22-4243-936a-f299d2d0428e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8892099-5daf-4d71-92a8-824f63c0a35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=balanced_data,\n",
    "    eval_dataset=tokenized_fold_dataset[\"sub_val\"],\n",
    "    tokenizer=fast_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac917b7-6f71-426c-840b-247d376b24bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f9447-11ef-47ac-9c09-d4afd2110cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f183fd-df2f-4943-8290-86c2e65a852c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1c98e-d978-4b45-afe5-9da0caa0c33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_fold_dataset['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f46daa-4bfc-4c29-96a6-d59f83e62857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(f\"./data/clean_model_composite_{frozen}_{split_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5aee14-a2ab-4634-b3ee-6c724078f637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
